\section{Related Work}
\label{sec:related}

\subsection{Rumor Tracking}
\label{sec:rumortracking}
The earliest Rumor tracking work is traced back to 2011. Several studies aim to find the relevant rumors for a known prior \cite{DBLP:journals/csur/ZubiagaABLP18}. Qazvinian et al. \cite{DBLP:conf/emnlp/QazvinianRRM11} explores three types of features including content-based, network-based, and tweet specific memes for identifying rumors. Hamidian et al. \cite{DBLP:journals/corr/abs-1912-08926} devise novel features and classify rumors with WEKA platform. These studies all focus on building features with simple classifiers. Cheng et al. \cite{DBLP:conf/www/ChengNB20} propose a multi-task learning model named VRoC and treat rumor tracking as one of the sub-tasks. VRoC is based on variational auto-encoder and uses tweet content as the input features. Overall, the rumor tracking task is an important sub-task in rumor detection. However, compared to the other three sub-tasks, it attracts less attention.

\subsection{Text Classification}
\label{sec:textclassification}
Text classification is a significant task in the NLP area. In recent years, deep learning based text classification gets well developed. Kim et al. \cite{DBLP:conf/emnlp/Kim14} propose CNN based text classification. TextCNN uses different sized kernels to capture different scaled gram features. Bojanowski et al. propose FastText  \cite{DBLP:journals/tacl/BojanowskiGJM17}, which gets embeddings of text in a short time. Inception \cite{DBLP:journals/corr/SzegedyLJSRAEVR14} uses larger width of network structure to capture more features. DPCNN \cite{DBLP:conf/acl/JohnsonZ17} adopts a deeper network structure to get a better performance. From the recent studies, we find that the deep learning model achieves great success on text classification tasks.
