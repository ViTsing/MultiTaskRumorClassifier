\section{Related Work}
\label{sec:related}

\subsection{Rumor Tracking}
\label{sec:rumortracking}
The earliest Rumor tracking work is traced back to 2011. \cite{DBLP:journals/csur/ZubiagaABLP18} aims to find the relevant rumors for a known prior. Qazvinian et al. \cite{DBLP:conf/emnlp/QazvinianRRM11} explore three types of features including content-based, network-based, and tweet specific memes for identifying rumors. Hamidian et al. \cite{DBLP:journals/corr/abs-1912-08926} devise novel features and classify rumors with WEKA platform. The previous studies mainly focus on building features with simple classifiers. Cheng et al. \cite{DBLP:conf/www/ChengNB20} propose a multi-task learning model named VRoC and treat rumor tracking as one of the sub-tasks. VRoC is based on variational auto-encoder and uses tweet content as the input features. Overall, the rumor tracking task is an important sub-task in rumor detection. However, compared to the other three sub-tasks, it attracts less attention.

\subsection{Text Classification}
\label{sec:textclassification}
Text classification is a significant task in NLP area. In recent years, deep learning based text classification develops rapidly. Kim et al. \cite{DBLP:conf/emnlp/Kim14} propose CNN based text classification. TextCNN uses different sized kernels to capture different scaled gram features. Bojanowski et al. propose FastText \cite{DBLP:journals/tacl/BojanowskiGJM17}, which generates embeddings of text in a short time. Inception \cite{DBLP:journals/corr/SzegedyLJSRAEVR14} uses larger width of network structure to capture more features. DPCNN \cite{DBLP:conf/acl/JohnsonZ17} adopts a deeper network structure to achieve a better performance. According to the recent studies, deep learning models achieve great success on text classification tasks.

\subsection{Deep Reinforcement Learning}
The most representative deep reinforcement learning model is DQN \cite{DBLP:journals/corr/MnihKSGAWR13}. As a value-based reinforcement learning method, DQN adds a convolutional neural network to Q-learning \cite{DBLP:journals/ml/WatkinsD92} and acquires a significant promotion. Then, many improvements \cite{DBLP:journals/nature/MnihKSRVBGRFOPB15, DBLP:conf/aaai/HasseltGS16} are proposed based on DQN. Besides value-based model, policy-based deep reinforcement learning models are proposed in recent years. Proximal Policy Optimization Algorithms (PPO) \cite{DBLP:journals/corr/SchulmanWDRK17} alternates between sampling data through interaction with the environment and adopts an optimization using stochastic gradient ascent. Deterministic Policy Gradient Algorithms (DPG) \cite{DBLP:conf/icml/SilverLHDWR14} adopts deterministic policy to replace the stochastic policy for actor network. We adopt policy-based model as our weight-tuning policy network to generate the weights of classifiers.